# EP5-S16.2: Advanced Lip-Sync with Audio Analysis

As a candidate,
I want the AI interviewer's avatar to have realistic lip movements synchronized with actual audio,
So that the interview experience feels natural and professional.

## Scope

- Implement real-time audio analysis for accurate lip-sync
- Map audio frequency data to ARKit viseme/phoneme blendshapes
- Smooth transitions between mouth shapes
- Maintain 60fps performance during audio playback
- Graceful fallback to basic animation when audio analysis unavailable

## Acceptance Criteria

1. Avatar mouth movements match audio phonemes within 50ms latency
2. Smooth interpolation between viseme targets (no jarring transitions)
3. Accurate mapping of common phonemes: A, E, I, O, U, M, F, TH, S
4. No audio glitches or stuttering during analysis
5. Performance: max +3% CPU usage compared to basic lip-sync
6. Works with both local and remote audio tracks
7. Fallback to sine-wave animation if Web Audio API unavailable

## Technical Stack

- **Web Audio API**: Audio analysis and frequency data
- **AnalyserNode**: Real-time FFT for frequency analysis
- **ARKit Blendshapes**: 52 facial morph targets (focus on mouth/jaw)
- **requestAnimationFrame**: Smooth 60fps updates
- **Linear Interpolation (LERP)**: Smooth transitions between shapes

## Implementation Plan

### 1. Audio Analyzer Setup

```typescript
// src/components/interview/v2/AudioAnalyzer.ts
export class AudioLipSyncAnalyzer {
  private audioContext: AudioContext;
  private analyser: AnalyserNode;
  private dataArray: Uint8Array;
  private source: MediaStreamAudioSourceNode | null = null;

  constructor(fftSize: number = 256) {
    this.audioContext = new AudioContext();
    this.analyser = this.audioContext.createAnalyser();
    this.analyser.fftSize = fftSize;
    this.dataArray = new Uint8Array(this.analyser.frequencyBinCount);
  }

  connectStream(stream: MediaStream): void {
    if (this.source) {
      this.source.disconnect();
    }
    this.source = this.audioContext.createMediaStreamSource(stream);
    this.source.connect(this.analyser);
  }

  getVisemeWeights(): VisemeWeights {
    this.analyser.getByteFrequencyData(this.dataArray);
    return this.mapFrequenciesToVisemes(this.dataArray);
  }

  private mapFrequenciesToVisemes(frequencies: Uint8Array): VisemeWeights {
    // Analyze frequency bands to detect phonemes
    const lowFreq = this.getAverageFrequency(frequencies, 0, 4); // 0-500Hz (vowels)
    const midFreq = this.getAverageFrequency(frequencies, 4, 12); // 500-1500Hz (consonants)
    const highFreq = this.getAverageFrequency(frequencies, 12, 24); // 1500-3000Hz (sibilants)

    const volume = this.getAverageFrequency(frequencies, 0, frequencies.length);
    const isSpeaking = volume > 30; // threshold

    if (!isSpeaking) {
      return { jawOpen: 0, mouthClose: 1, mouthPucker: 0, mouthSmile: 0 };
    }

    // Map frequency patterns to mouth shapes
    return {
      jawOpen: this.normalize(lowFreq, 0, 150) * 0.8,
      mouthClose: midFreq < 50 ? 1 : 0,
      mouthPucker: this.normalize(midFreq, 60, 120) * 0.6,
      mouthSmile: this.normalize(highFreq, 80, 140) * 0.5,
      mouthFunnel: this.normalize(lowFreq, 100, 180) * 0.4,
      // Add more ARKit visemes as needed
    };
  }

  private getAverageFrequency(
    data: Uint8Array,
    startIdx: number,
    endIdx: number
  ): number {
    let sum = 0;
    for (let i = startIdx; i < Math.min(endIdx, data.length); i++) {
      sum += data[i];
    }
    return sum / (endIdx - startIdx);
  }

  private normalize(value: number, min: number, max: number): number {
    return Math.max(0, Math.min(1, (value - min) / (max - min)));
  }

  dispose(): void {
    if (this.source) {
      this.source.disconnect();
    }
    this.analyser.disconnect();
    this.audioContext.close();
  }
}

export interface VisemeWeights {
  jawOpen?: number;
  mouthClose?: number;
  mouthPucker?: number;
  mouthSmile?: number;
  mouthFunnel?: number;
  mouthLeft?: number;
  mouthRight?: number;
  [key: string]: number | undefined;
}
```

### 2. Avatar Component Integration

```typescript
// Update AIAvatarCanvas.tsx
const Avatar: React.FC<AvatarProps> = ({ isSpeaking, audioStream }) => {
  const analyzerRef = useRef<AudioLipSyncAnalyzer | null>(null);
  const visemeWeightsRef = useRef<VisemeWeights>({});

  // Initialize audio analyzer when stream becomes available
  useEffect(() => {
    if (audioStream && !analyzerRef.current) {
      try {
        analyzerRef.current = new AudioLipSyncAnalyzer(256);
        analyzerRef.current.connectStream(audioStream);
        console.log('[Avatar] Audio analyzer initialized');
      } catch (err) {
        console.error('[Avatar] Failed to initialize audio analyzer:', err);
      }
    }

    return () => {
      analyzerRef.current?.dispose();
      analyzerRef.current = null;
    };
  }, [audioStream]);

  // Animation loop with audio-driven lip-sync
  useFrame(state => {
    if (!groupRef.current || !scene) return;

    const elapsed = state.clock.elapsedTime;

    // Breathing animation
    groupRef.current.position.y = Math.sin(elapsed * 0.5) * 0.02;

    if (isSpeaking && analyzerRef.current && morphMeshRef.current) {
      // Get current viseme weights from audio analysis
      const weights = analyzerRef.current.getVisemeWeights();
      visemeWeightsRef.current = weights;

      const dict = morphMeshRef.current.morphTargetDictionary;
      const influences = morphMeshRef.current.morphTargetInfluences;

      if (dict && influences) {
        // Smoothly interpolate morph targets
        Object.entries(weights).forEach(([viseme, targetWeight]) => {
          const idx = dict[viseme];
          if (idx !== undefined && targetWeight !== undefined) {
            // LERP for smooth transitions (0.3 = interpolation speed)
            influences[idx] = THREE.MathUtils.lerp(
              influences[idx] || 0,
              targetWeight,
              0.3
            );
          }
        });
      }
    } else {
      // Idle: reset mouth shapes
      if (morphMeshRef.current?.morphTargetInfluences) {
        const dict = morphMeshRef.current.morphTargetDictionary;
        const influences = morphMeshRef.current.morphTargetInfluences;

        if (dict && influences) {
          // Smoothly return to neutral
          ['jawOpen', 'mouthPucker', 'mouthSmile', 'mouthFunnel'].forEach(
            viseme => {
              const idx = dict[viseme];
              if (idx !== undefined) {
                influences[idx] = THREE.MathUtils.lerp(
                  influences[idx] || 0,
                  0,
                  0.2
                );
              }
            }
          );
          // Set mouthClose to 1
          const closeIdx = dict['mouthClose'];
          if (closeIdx !== undefined) {
            influences[closeIdx] = THREE.MathUtils.lerp(
              influences[closeIdx] || 0,
              1,
              0.2
            );
          }
        }
      }
    }
  });
};
```

### 3. ModernInterviewPage Integration

```typescript
// Update ModernInterviewPage.tsx to pass audio stream
const ModernInterviewPage: React.FC<Props> = ({ applicationId }) => {
  const [remoteAudioStream, setRemoteAudioStream] = useState<MediaStream | null>(null);

  useEffect(() => {
    const handleRemoteStream = () => {
      const w = window as unknown as { __interviewV2RemoteStream?: MediaStream };
      if (w.__interviewV2RemoteStream) {
        setRemoteAudioStream(w.__interviewV2RemoteStream);
      }
    };

    window.addEventListener('interview:remote_stream_ready', handleRemoteStream);
    return () => window.removeEventListener('interview:remote_stream_ready', handleRemoteStream);
  }, []);

  return (
    <AIAvatarCanvas
      isSpeaking={controller.state.aiSpeaking ?? false}
      audioStream={remoteAudioStream}
    />
  );
};
```

## Viseme Mapping Reference

ARKit blendshapes to phoneme mapping:

| Phoneme | Example  | ARKit Blendshape            | Frequency Range |
| ------- | -------- | --------------------------- | --------------- |
| AA (ah) | "father" | jawOpen + mouthOpen         | 500-700Hz       |
| EE (ee) | "see"    | mouthSmile + jawOpen(small) | 2000-2500Hz     |
| OO (oo) | "you"    | mouthPucker + mouthFunnel   | 300-500Hz       |
| AE (a)  | "cat"    | jawOpen + mouthSmile        | 700-1000Hz      |
| M/B/P   | "mom"    | mouthClose + jawOpen(0)     | Low energy      |
| F/V     | "five"   | mouthLowerDownLeft/Right    | 1500-2000Hz     |
| S/Z     | "say"    | mouthSmile + high freq      | 4000-6000Hz     |
| TH      | "the"    | jawOpen(small) + tongue     | 1500-2500Hz     |

## Performance Optimizations

1. **Throttle Analysis**: Run FFT at 60fps, not for every audio sample
2. **Buffer Pooling**: Reuse Uint8Array buffers
3. **Lazy Init**: Only create AudioContext when audio stream available
4. **Dispose Properly**: Clean up Web Audio resources on unmount
5. **Worker Thread**: Consider moving FFT to Web Worker if CPU constrained

## Fallback Strategy

```typescript
// Graceful degradation hierarchy:
// 1. Audio analysis with ARKit blendshapes (best)
// 2. Audio analysis with basic jaw open (good)
// 3. Sine wave jaw animation (acceptable)
// 4. Static mouth closed (minimal)

if (analyzerAvailable && arkitBlendshapesAvailable) {
  useAudioDrivenBlendshapes();
} else if (analyzerAvailable && jawBoneAvailable) {
  useAudioDrivenJawRotation();
} else if (arkitBlendshapesAvailable) {
  useSineWaveBlendshapes(); // Current EP5-S16 implementation
} else {
  useSineWaveJawRotation();
}
```

## Edge Cases

- **Audio Context Suspended**: Resume on user interaction
- **CORS Issues**: Audio stream from different origin may fail
- **High CPU**: Reduce FFT size from 256 to 128
- **No Audio Stream**: Fall back to basic animation
- **Multiple Streams**: Handle stream switches gracefully
- **Mobile Safari**: May require AudioContext unlock via user gesture

## Tests

- Unit: VisemeWeights calculation from frequency data
- Integration: Audio stream → analyzer → morph targets
- Performance: Measure CPU usage with/without analysis
- Visual: Side-by-side comparison of audio-driven vs sine-wave
- Regression: Ensure basic lip-sync still works without audio stream

## Definition of Done

Avatar lip movements accurately reflect audio phonemes with <50ms latency. Smooth transitions between mouth shapes. No audio glitches. Performance within 3% CPU overhead. Graceful fallback when audio analysis unavailable.

## Tasks

- [ ] Implement `AudioLipSyncAnalyzer` class with FFT analysis
- [ ] Map frequency bands to ARKit viseme weights
- [ ] Integrate analyzer into `Avatar` component
- [ ] Add LERP for smooth morph target transitions
- [ ] Pass remote audio stream from `ModernInterviewPage`
- [ ] Test with various phoneme combinations
- [ ] Measure and optimize performance
- [ ] Implement fallback hierarchy
- [ ] Add error handling for Web Audio API failures
- [ ] Document viseme mapping for future tuning

## Dependencies

- **Blocked by**: EP5-S16 (basic avatar integration must be complete)
- **Blocks**: None

## Related Stories

- EP5-S16: ReadyPlayerMe Avatar Integration (prerequisite)
- EP5-S16.1: Dynamic Avatar Selection (parallel enhancement)

## Resources

- [Web Audio API - MDN](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- [ARKit Blendshapes Reference](https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation)
- [Viseme to Phoneme Mapping](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-speech-synthesis-viseme)
- [Three.js Morph Target Animation](https://threejs.org/docs/#manual/en/introduction/Animation-system)
